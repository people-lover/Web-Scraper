{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PDF Downloader"
=======
   "metadata": {},
   "source": [
    "# Downlaoding PDFS from a website"
>>>>>>> 412d53eea3bc3d4b2eac6aedfecf06172def6b2b
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    "# Libraries Used\n",
    "\n",
    "### urllib\n",
    "\n",
    "For opening URLs\n",
    "\n",
    "To install: \n",
    "pip install urllib\n",
    "\n",
    "### bs4\n",
    "\n",
    "For Scraping the page using BeautifulSoup\n",
    "\n",
    "To install: \n",
    "pip install bs4"
=======
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Import the Modules\n",
    "So this typically parses the webpage and downloads all the pdfs in it. I used BeautifulSoup but you can use mechanize or whatever you want."
>>>>>>> 412d53eea3bc3d4b2eac6aedfecf06172def6b2b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing files\n",
    "\n",
    "'''#Python 3\n",
    "#importing dependencies\n",
    "from urllib.request import urlopen as uReq #for opening URLs\n",
    "from bs4 import BeautifulSoup as soup #for scraping the web'''\n",
    "\n",
    "#Python 2\n",
    "#importing dependencies\n",
    "from urllib2 import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPDFurls(url):\n",
    "    print (url)\n",
    "    \n",
    "    my_url = url #URL you want to scrap\n",
    "\n",
    "    uClient = uReq(my_url) #opening connection and grabing the page\n",
    "    page_html = uClient.read() #reading the page\n",
    "    uClient.close() #closing the connection\n",
    "\n",
    "    #parsing as html (can be xml,etc.)\n",
    "    page_soup = soup(page_html, \"html.parser\") \n",
    "\n",
    "    #grabing all <A> tags\n",
    "    aAll = page_soup.findAll(\"a\")\n",
    "    \n",
    "    pdf_url_list = []\n",
    "    \n",
    "    #going through all <A> tags\n",
    "    for a in aAll:\n",
    "        link = a[\"href\"] #getting the href attribute or the URL\n",
    "        if (link[-4:] == \".pdf\"): #checking whether it is a pdf url or not\n",
    "            pdf_url_list.append(link)\n",
    "    \n",
    "    #returning the url list\n",
    "    return (pdf_url_list)\n"
=======
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import urlparse\n",
    "import urllib2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#checking if BeautifulSoup is present\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ImportError:\n",
    "    print \"[*] Please download and install Beautiful Soup: \\n Needed for reading website code.\\n\\n Use \\\"pip install bs4\\\" to install it.\"\n",
    "#bs4 is not automatically installed with python\n",
    "#You need to install it bu opening cmd and type\n",
    "#pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data\n",
    "Now you enter your data like your URL(that contains the pdfs) and the download path(where the pdfs will be saved) also I added headers to make it look a bit legit...but you can add yours...it's not really necessary though. Also the BeautifulSoup is to parse the webpage for links"
>>>>>>> 412d53eea3bc3d4b2eac6aedfecf06172def6b2b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadPDFurls(url):\n",
    "    print (url)\n",
    "    \n",
    "    pdf_url_list = getPDFurls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://football-data.co.uk/englandm.php\n",
      "[u'http://www.football-data.co.uk/ratings.pdf', u'http://www.football-data.co.uk/ratings.pdf']\n"
=======
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected unindent (<ipython-input-15-2817b2667426>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-2817b2667426>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    soup = BeautifulSoup(html.read())#to parse the website\u001b[0m\n\u001b[1;37m                                                          ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected unindent\n"
     ]
    }
   ],
   "source": [
    "def readURL(url):\n",
    "    url = \"\" #input\n",
    "    downoad_path = \"\" #input\n",
    "    try:\n",
    "        #to make it look legit for the url\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; rv:38.0) Gecko/20100101 Firefox/38.0\"}\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        request = urllib2.Request(url, None, headers)\n",
    "        html = urllib2.urlopen(request)\n",
    "        soup = BeautifulSoup(html.read())#to parse the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Main Program\n",
    "This part of the program is where it actually parses the webpage for links and checks if it has a pdf extension and then downloads it. I also added a counter so you know how many pdfs have been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ae805944bc5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m#Find <a> tags with href in it so you know it is for urls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#if it doesn't contain the full url it can be the url itself to it for the download\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
>>>>>>> 412d53eea3bc3d4b2eac6aedfecf06172def6b2b
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print (getPDFurls(\"http://football-data.co.uk/englandm.php\"))"
=======
    "for tag in soup.findAll ('a', href = True): \n",
    "    #Find <a> tags with href in it so you know it is for urls\n",
    "    #if it doesn't contain the full url it can be the url itself to it for the download\n",
    "    tag['href'] = urlparse.urljoin(url, tag['href'])\n",
    "        \n",
    "    #We are getting the extension (splittext) from the last name of the full url(basename)\n",
    "    #The splitext splits it into the filename and the extension so the [1] is for the second part (the extension)\n",
    "    if os.path.splitext (os.path.basename(tag['href']))[1] == '.pdf':\n",
    "        current = urllib2.urlopen(tag['href'])\n",
    "        print (\"\\n[*] Downloading: %s\" %(os.path.basename(tag['href'])))\n",
    "        \n",
    "        f = open(doenload_path + \"\\\\\" + os.path.basename(tag['href'],\"wb\"))\n",
    "        f.write(current.read())\n",
    "        f.close()\n",
    "        i += 1\n",
    "        \n",
    "print (\"\\n[*] Downloaded %d files\" %(i*1))\n",
    "raw_input (\"[+] Press any key to exit...\")"
>>>>>>> 412d53eea3bc3d4b2eac6aedfecf06172def6b2b
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### Kuldeep Singh Sidhu\n",
    "##### github.com/singhsidhukuldeep"
=======
    "## Taking Care of Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "except KeyboardInterrupt:\n",
    "    print \"[*] Exiting\"\n",
    "    sys.exit(1)\n",
    "    \n",
    "except URLError as e:\n",
    "    print (\"[*] Could not get information from the server!\")\n",
    "    sys.exit(2)\n",
    "    \n",
    "except:\n",
    "    print (\"An error occured while executing the program\")\n",
    "    sys.exit(3)"
>>>>>>> 412d53eea3bc3d4b2eac6aedfecf06172def6b2b
   ]
  }
 ],
 "metadata": {
<<<<<<< HEAD
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
=======
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
>>>>>>> 412d53eea3bc3d4b2eac6aedfecf06172def6b2b
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
<<<<<<< HEAD
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
=======
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
>>>>>>> 412d53eea3bc3d4b2eac6aedfecf06172def6b2b
}
